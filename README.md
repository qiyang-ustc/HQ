# HQ (HeadQuarters) - Cluster Job Submission Tool ğŸš€

HQ is a powerful, user-friendly command-line tool designed to simplify job submission and management across different HPC (High-Performance Computing) clusters. It provides a unified interface for submitting jobs with different resource requirements, supports parameter sweeps, and offers a safe preview mode.

HQ is completely written by LLM, I did not write any single word of this code.

## Features âœ¨

- ğŸ”„ **Submit Multiple Jobs**: Submit single or multiple jobs with different resource requirements
- ğŸ“Š **Parameter Sweep**: Easy parameter sweep support with template expansion
- ğŸŒ **Multi-Cluster Support**: Configure and use multiple clusters with customizable settings
- ğŸ‘€ **Preview Mode**: Verify commands before execution with the `-p` option
- ğŸ¨ **Colorized Output**: Better readability with color-coded command output
- âš™ï¸ **Auto-Config**: Automatic configuration file generation with examples
- ğŸ” **Interactive Debug**: Support for debug and interactive sessions
- ğŸ’¾ **Resource Management**: Flexible CPU and memory allocation

## Quick Start ğŸš¦

1. **First-time Setup**:
   ```bash
   # Generate a sample configuration file
   ./hq --generate-config
   
   # Edit the generated config file with your cluster settings
   vim ~/.cluster_profile
   ```

2. **Basic Usage**:
   ```bash
   # Always preview commands first (recommended)
   ./hq -p myfile.py -c 4 -m 8
   
   # Submit the job if preview looks good
   ./hq myfile.py -c 4 -m 8
   ```

## Usage Examples ğŸ“š

### Basic Job Submission
```bash
# Submit a single job with 4 cores and 8GB memory
./hq myfile.py -c 4 -m 8

# Submit multiple jobs with different resources
./hq file1.py file2.py -c 4,8 -m 8,16
```

### Parameter Sweep
```bash
# Create and submit jobs for different parameters
./hq -c 4,4 -m 8,8 -l 4,5 myfile_D{}
# This will create and submit: myfile_D4 and myfile_D5
```

### Cluster-specific Options
```bash
# Submit to a specific cluster
./hq myfile.py -c 4 -m 8 --cluster snellius

# Use GPU partition
./hq myfile.py -c 4 -m 8 -t gpu

# Request interactive debug session
./hq myfile.py -c 4 -m 8 -t debug

# Use bigmem partition for large memory jobs
./hq myfile.py -c 4 -m 8 -t bigmem
```

## Configuration ğŸ”§

HQ uses a YAML configuration file (`~/.cluster_profile`) to store cluster settings. Here's an example configuration:

```yaml
default_cluster: "example_cluster"  # Default cluster to use
default_cores: 2                    # Default CPU cores
default_memory: 5                   # Default memory in GB

clusters:
  example_cluster:
    partition: "compute"            # Default partition
    remote_dir: "/home/username/projects"  # Remote working directory
    bigmem_threshold: 32           # Memory threshold for bigmem partition
    targets:
      # Job submission templates
      default: "sbatch --partition={partition} --cpus-per-task={cores} --mem={memory} --job-name={filename} ~/bin/runjob {directory} {filename}"
      gpu: "sbatch --partition=gpu --cpus-per-task={cores} --mem={memory} --gres=gpu:1 --job-name={filename} ~/bin/runjob {directory} {filename}"
      debug: "srun --partition={partition} --cpus-per-task={cores} --mem={memory} ~/bin/runjob {directory} {filename}"
      bigmem: "sbatch --partition=bigmem --cpus-per-task={cores} --mem={memory} --job-name={filename} ~/bin/runjob {directory} {filename}"
```

## Command-line Options ğŸ› ï¸

| Option | Description |
|--------|-------------|
| `-p, --preview` | ğŸ‘‰ Preview commands without executing (Recommended!) |
| `-c, --cores` | Number of CPU cores (e.g., '4' or '4,8') |
| `-m, --memory` | Memory in GB (e.g., '8' or '8,16') |
| `-l, --values` | Values for template expansion (e.g., '4,5,6') |
| `-t, --target` | Target configuration (default/gpu/debug/bigmem) |
| `--cluster` | Target cluster name |
| `--config` | Custom config file path |
| `-v, --verbose` | Enable verbose output |

## Best Practices ğŸ’¡

1. **Always Preview First**: Use `-p` to verify commands before actual execution
2. **Resource Planning**: 
   - Start with small resource requests and scale up as needed
   - Use `bigmem` partition for high-memory jobs
3. **Parameter Sweeps**: 
   - Use template expansion for systematic parameter studies
   - Keep filenames descriptive of parameters
4. **Interactive Development**:
   - Use `-t debug` for interactive sessions during development
   - Switch to batch jobs for production runs

## Troubleshooting ğŸ”

1. **No Configuration File**:
   ```bash
   ./hq --generate-config
   ```

2. **Command Preview**:
   ```bash
   ./hq -p -v myfile.py  # Use both preview and verbose mode
   ```

3. **Common Issues**:
   - Ensure remote directories exist
   - Check file permissions
   - Verify cluster names in config match actual clusters
   - Ensure template values match the number of files

## Requirements ğŸ“‹

- Python 3.6+
- Required Python packages:
  - `pyyaml`
  - `colorama`
  - `typing`

## License ğŸ“„

MIT License - Feel free to use and modify as needed!